{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafbb2fa",
   "metadata": {},
   "source": [
    "# RDD Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5837af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"TestingRDD\").getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed08515",
   "metadata": {},
   "source": [
    "### Define unstructured list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204f3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = \"Spark makes life a lot easier and put me into good Spirits, Spark is too Awesome!\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90450f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'makes', 'life', 'a', 'lot', 'easier', 'and', 'put', 'me', 'into', 'good', 'Spirits,', 'Spark', 'is', 'too', 'Awesome!']\n"
     ]
    }
   ],
   "source": [
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d58a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00797a",
   "metadata": {},
   "source": [
    "### Creating RDD from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d05747",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = spark.sparkContext.parallelize(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b37aa",
   "metadata": {},
   "source": [
    "**collect()** action is used to retrieve all the data from a distributed RDD or DataFrame and return it as a local collection in the driver program. The purpose of collect() is to bring the distributed data into the driver program's memory so that you can work with it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2904704",
   "metadata": {},
   "source": [
    "When you apply transformations on an RDD or DataFrame, the data is processed in a distributed manner across multiple worker nodes in the Spark cluster. The intermediate and final results are stored in memory across these nodes. However, there might be scenarios where you need to access or analyze the complete dataset locally within the driver program. That's when the **collect()** action comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4089a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = words_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10366e",
   "metadata": {},
   "source": [
    "### Printing for each data in RDD words_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74407e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "easier\n",
      "and\n",
      "put\n",
      "me\n",
      "into\n",
      "good\n",
      "Spirits,\n",
      "Spark\n",
      "is\n",
      "too\n",
      "Awesome!\n"
     ]
    }
   ],
   "source": [
    "for word in words_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3326014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e33e1f",
   "metadata": {},
   "source": [
    "## Transformation in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e440588",
   "metadata": {},
   "source": [
    "### Apply distinct function into RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b67e7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5ce9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "easier\n",
      "and\n",
      "put\n",
      "me\n",
      "into\n",
      "good\n",
      "Spirits,\n",
      "Spark\n",
      "is\n",
      "too\n",
      "Awesome!\n"
     ]
    }
   ],
   "source": [
    "words_data = words_rdd.collect()\n",
    "for word in words_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55021ba",
   "metadata": {},
   "source": [
    "When iterating again the words_data the word 'Spark' not distincted even after apply the function, because data in RDD is immutable. and function distinct() is not creating a new RDD which hold distinctive value, unless we defined a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5a1859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put\n",
      "good\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "and\n",
      "Awesome!\n",
      "Spark\n",
      "into\n",
      "Spirits,\n",
      "is\n",
      "easier\n",
      "me\n",
      "too\n"
     ]
    }
   ],
   "source": [
    "words_unique_rdd = words_rdd.distinct()\n",
    "words_unique_data = words_unique_rdd.collect()\n",
    "for word in words_unique_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4bb62",
   "metadata": {},
   "source": [
    "now duplicated record is removed after re defined RDD to hold distinctive value of the word list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebac79b",
   "metadata": {},
   "source": [
    "### Apply filter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61a85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsStartsWith(word, letter):\n",
    "    return word.startswith(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e82174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Spirits,', 'Spark']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.filter(lambda word: wordsStartsWith(word,'S')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52973423",
   "metadata": {},
   "source": [
    "### Apply map function (transfrom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d656c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "numbers = [*range(1,20)]\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596bcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_numbers = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88231bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_rdd_numbers = rdd_numbers.map(lambda x : (x, x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2189123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(2, 4)\n",
      "(3, 9)\n",
      "(4, 16)\n",
      "(5, 25)\n",
      "(6, 36)\n",
      "(7, 49)\n",
      "(8, 64)\n",
      "(9, 81)\n",
      "(10, 100)\n",
      "(11, 121)\n",
      "(12, 144)\n",
      "(13, 169)\n",
      "(14, 196)\n",
      "(15, 225)\n",
      "(16, 256)\n",
      "(17, 289)\n",
      "(18, 324)\n",
      "(19, 361)\n"
     ]
    }
   ],
   "source": [
    "for num in squared_rdd_numbers.collect():\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb9e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_trd_rdd = words_rdd.map(lambda word:(word,word[0],wordsStartsWith(word,'S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_trd_rdd.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8cd84",
   "metadata": {},
   "source": [
    "### Sorted by List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fb0ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_list = [(\"India\",91),(\"USA\",4),(\"Greece\",13)]\n",
    "countries_rdd = spark.sparkContext.parallelize(countries_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ded4d",
   "metadata": {},
   "source": [
    "By Default sorted by Country ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b5ddb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_countries_list = countries_rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3158f632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greece', 13)\n",
      "('India', 91)\n",
      "('USA', 4)\n"
     ]
    }
   ],
   "source": [
    "for country in sorted_countries_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839bc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_countries_list = countries_rdd.map(lambda c: (c[1],c[0])).sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b24df",
   "metadata": {},
   "source": [
    "After swapping value into new tuples, it now sorted by rank instead in Descending order (set Sorted key False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83fa8de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 'India')\n",
      "(13, 'Greece')\n",
      "(4, 'USA')\n"
     ]
    }
   ],
   "source": [
    "for country in sorted_countries_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d0672",
   "metadata": {},
   "source": [
    "## Action in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30503820",
   "metadata": {},
   "source": [
    "### Reduce Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db79ef1",
   "metadata": {},
   "source": [
    "The reduce function in PySpark RDD is used to aggregate the elements of an RDD using a binary operator. It takes a binary operator function as an argument, which combines two elements at a time to produce a single result.\n",
    "\n",
    "The reduce function in PySpark RDD does not require the use of collect.\n",
    "Unlike transformations like map or filter that return a new RDD, the reduce operation directly returns the final result. Therefore, there is no need to use collect after reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1889995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "numbers_rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "sum_of_numbers = numbers_rdd.reduce(lambda x, y: x + y)\n",
    "print (sum_of_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192c74a",
   "metadata": {},
   "source": [
    "**Explain behind the reduce lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b845a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumList(x,y):\n",
    "    print(x,y)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7e9bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 3\n",
      "6 9\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "result = numbers_rdd.reduce(lambda x, y: sumList(x,y))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74e1f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    word=\"\"\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        word=leftWord\n",
    "    else:\n",
    "        word=rightWord\n",
    "    #print(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56beb0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Awesome!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e1de7",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed6091",
   "metadata": {},
   "source": [
    "### Finding Max/Min value in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97f9c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 9\n"
     ]
    }
   ],
   "source": [
    "numbers_rdd = spark.sparkContext.parallelize([7, 2, 9, 1, 5, 3])\n",
    "max_value = numbers_rdd.reduce(lambda x, y: x if x > y else y)\n",
    "print(\"Maximum value:\", max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768953ec",
   "metadata": {},
   "source": [
    "Let's illustrate each iteration of the reduce operation for finding the maximum value in the RDD [7, 2, 9, 1, 5, 3]:\n",
    "\n",
    "Iteration 1:\n",
    "\n",
    "x = 7 (initial accumulated maximum)\n",
    "y = 2 (next element in the RDD)\n",
    "Since x (7) is greater than y (2), the lambda function returns x.\n",
    "Accumulated maximum after the first iteration: 7\n",
    "Iteration 2:\n",
    "\n",
    "x = 7 (accumulated maximum from the previous iteration)\n",
    "y = 9 (next element in the RDD)\n",
    "Since y (9) is greater than x (7), the lambda function returns y.\n",
    "Accumulated maximum after the second iteration: 9\n",
    "Iteration 3:\n",
    "\n",
    "x = 9 (accumulated maximum from the previous iteration)\n",
    "y = 1 (next element in the RDD)\n",
    "Since x (9) is greater than y (1), the lambda function returns x.\n",
    "Accumulated maximum after the third iteration: 9\n",
    "Iteration 4:\n",
    "\n",
    "x = 9 (accumulated maximum from the previous iteration)\n",
    "y = 5 (next element in the RDD)\n",
    "Since x (9) is greater than y (5), the lambda function returns x.\n",
    "Accumulated maximum after the fourth iteration: 9\n",
    "Iteration 5:\n",
    "\n",
    "x = 9 (accumulated maximum from the previous iteration)\n",
    "y = 3 (next element in the RDD)\n",
    "Since x (9) is greater than y (3), the lambda function returns x.\n",
    "Accumulated maximum after the fifth iteration: 9\n",
    "After iterating through all elements in the RDD, the final result is 9, which is the maximum value in the RDD [7, 2, 9, 1, 5, 3]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b11745",
   "metadata": {},
   "source": [
    "### Daily Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "146c739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ferenheit_temp = spark.sparkContext.parallelize([59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4])\n",
    "celcius_temp = ferenheit_temp.map(lambda x: (x-32)*(5/9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57a601bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "14.000000000000002\n",
      "12.000000000000002\n",
      "13.0\n",
      "10.999999999999998\n",
      "12.000000000000002\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "for temp in celcius_temp.collect():\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bb5507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "14.000000000000002\n",
      "13.0\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "filterTempRDD = celcius_temp.filter(lambda x: x >= 13)\n",
    "for temp in filterTempRDD.collect():\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a2e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
